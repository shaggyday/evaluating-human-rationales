{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils import avg, evidence_to_mask\n",
    "\n",
    "def to_data_df(df, data_dir):\n",
    "    data_df = []\n",
    "    columns = ['text', 'classification', 'rationale', 'query']\n",
    "    for i in tqdm(range(len(df))):\n",
    "        df_row = df.loc[i]\n",
    "        text_id = df_row['docids'][0]\n",
    "        query = df_row['query']\n",
    "        evidence_list = df_row['evidences']\n",
    "        if evidence_list:\n",
    "            evidence_list = [x for xx in evidence_list for x in xx]\n",
    "        classification = df_row['classification']\n",
    "        \n",
    "        file = f'{data_dir}/docs/{text_id}'\n",
    "        if file[-1] == '.': file = file[:-1] + '_'\n",
    "#         print(file)\n",
    "        if os.path.isfile(file):\n",
    "            f = open(file, 'r', encoding=\"utf-8\") \n",
    "            text = ''\n",
    "            for line in f.readlines():\n",
    "                text += line.rstrip() + ' '\n",
    "        else:\n",
    "            print(\"???\")      \n",
    "        \n",
    "        tokens = text.split()\n",
    "        rationale_mask = evidence_to_mask(tokens, evidence_list)\n",
    "        \n",
    "        # joining text and query with [SEP]\n",
    "        QA = f\"{query} [SEP] {text}\"\n",
    "        rationale_mask = [1]*(len(query.split())+1) + rationale_mask\n",
    "        \n",
    "        data_df.append([QA, classification, rationale_mask, query])\n",
    "    data_df = pd.DataFrame(data_df, columns=columns)\n",
    "    return data_df\n",
    "\n",
    "#     data_df_shuffled=data_df.sample(frac=1).reset_index(drop=True)\n",
    "#     return data_df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"fever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'../data/{dataset}'\n",
    "out_dir = f'others/{dataset}'\n",
    "train = pd.read_json(f'{data_dir}/train.jsonl', lines=True)\n",
    "test = pd.read_json(f'{data_dir}/test.jsonl', lines=True)\n",
    "val = pd.read_json(f'{data_dir}/val.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 97957/97957 [01:58<00:00, 825.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6111/6111 [00:06<00:00, 884.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6122/6122 [00:06<00:00, 905.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_df = to_data_df(train, data_dir)\n",
    "train_data_df.to_csv(f\"{out_dir}/train.csv\",index_label=\"id\")\n",
    "test_data_df = to_data_df(test, data_dir)\n",
    "test_data_df.to_csv(f\"{out_dir}/test.csv\",index_label=\"id\")\n",
    "val_data_df = to_data_df(val, data_dir)\n",
    "val_data_df.to_csv(f\"{out_dir}/val.csv\",index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationale_check(text,rationale):\n",
    "    tokens = text.split()\n",
    "    out = \"\"\n",
    "    for i, b in enumerate(rationale):\n",
    "        if b:\n",
    "           out += tokens[i] + \" \"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = test_data_df\n",
    "import json\n",
    "\n",
    "def reduce_by_alpha(text, rationale, fidelity_type=\"sufficiency\"):\n",
    "    reduced_text = \"\"\n",
    "    # whitespace tokenization\n",
    "    tokens = text.split()\n",
    "\n",
    "    for idx in range(len(tokens)):\n",
    "        try:\n",
    "            if fidelity_type == \"sufficiency\" and rationale[idx] >= 0.5:\n",
    "                reduced_text = reduced_text + tokens[idx] + \" \"\n",
    "            elif fidelity_type == \"comprehensiveness\" and rationale[idx] < 0.5:\n",
    "                reduced_text = reduced_text + tokens[idx] + \" \"\n",
    "        except Exception as e:\n",
    "            if fidelity_type == \"comprehensiveness\":\n",
    "                reduced_text = reduced_text + tokens[idx] + \" \"\n",
    "\n",
    "    # removed the last space from the text\n",
    "    if len(reduced_text) > 0:\n",
    "        reduced_text = reduced_text[:-1]\n",
    "\n",
    "    return reduced_text\n",
    "\n",
    "data_df = data_df[data_df['rationale'].notna()]\n",
    "data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "data_df[\"sufficiency_text\"] = data_df[\n",
    "    [\"text\", \"rationale\"]].apply(lambda s: reduce_by_alpha(*s, fidelity_type=\"sufficiency\"), axis=1)\n",
    "data_df[\"comprehensiveness_text\"] = data_df[\n",
    "    [\"text\", \"rationale\"]].apply(lambda s: reduce_by_alpha(*s, fidelity_type=\"comprehensiveness\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,000 genomes project enables mapping of genetic sequence variation consisting of rare variants with larger penetrance effects than common variants. [SEP] We propose as an alternative explanation that variants much less common than the associated one may create \" synthetic associations \" by occurring , stochastically , more often in association with one of the alleles at the common site versus the other allele . We show that they are not only possible , but inevitable , and that under simple but reasonable genetic models , they are likely to account for or contribute to many of the recently identified signals reported in genome-wide association studies . In conclusion , uncommon or rare genetic variants can easily create synthetic associations that are credited to common variants , and this possibility requires careful consideration in the interpretation and follow up of GWAS signals .'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['sufficiency_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IZombie premiered in 2015. [SEP] The series premiered on March 17 , 2015 . '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rationale_check(train_data_df.iloc[2]['text'],train_data_df.iloc[2]['rationale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>classification</th>\n",
       "      <th>docids</th>\n",
       "      <th>evidences</th>\n",
       "      <th>query</th>\n",
       "      <th>query_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[Ireland]</td>\n",
       "      <td>[[{'docid': 'Ireland', 'end_sentence': 7, 'end...</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Taylor_Schilling]</td>\n",
       "      <td>[[{'docid': 'Taylor_Schilling', 'end_sentence'...</td>\n",
       "      <td>The drama Dark Matter stars Taylor Schilling.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[IZombie_-LRB-TV_series-RRB-]</td>\n",
       "      <td>[[{'docid': 'IZombie_-LRB-TV_series-RRB-', 'en...</td>\n",
       "      <td>IZombie premiered in 2015.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100005</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Ronald_Reagan]</td>\n",
       "      <td>[[{'docid': 'Ronald_Reagan', 'end_sentence': 1...</td>\n",
       "      <td>Ronald Reagan had a nationality.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100006</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Samoa_Joe]</td>\n",
       "      <td>[[{'docid': 'Samoa_Joe', 'end_sentence': 1, 'e...</td>\n",
       "      <td>Samoa Joe wrestles professionally.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97952</th>\n",
       "      <td>99990</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[The_Simpsons]</td>\n",
       "      <td>[[{'docid': 'The_Simpsons', 'end_sentence': 12...</td>\n",
       "      <td>The Simpsons have aired on Fox for 29 seasons.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97953</th>\n",
       "      <td>99992</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[William_Shatner]</td>\n",
       "      <td>[[{'docid': 'William_Shatner', 'end_sentence':...</td>\n",
       "      <td>William Shatner did not host a show that won a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97954</th>\n",
       "      <td>99995</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Pablo_Fenjves]</td>\n",
       "      <td>[[{'docid': 'Pablo_Fenjves', 'end_sentence': 7...</td>\n",
       "      <td>Pablo Fenjves ghostwrote for a acting agent.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97955</th>\n",
       "      <td>99997</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[Dwight_D._Eisenhower]</td>\n",
       "      <td>[[{'docid': 'Dwight_D._Eisenhower', 'end_sente...</td>\n",
       "      <td>The Republic of China was recognized as China'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97956</th>\n",
       "      <td>99999</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[Jesus]</td>\n",
       "      <td>[[{'docid': 'Jesus', 'end_sentence': 4, 'end_t...</td>\n",
       "      <td>Jesus did not begin his own ministry.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97957 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       annotation_id classification                         docids  \\\n",
       "0                 10        REFUTES                      [Ireland]   \n",
       "1             100002       SUPPORTS             [Taylor_Schilling]   \n",
       "2             100004       SUPPORTS  [IZombie_-LRB-TV_series-RRB-]   \n",
       "3             100005       SUPPORTS                [Ronald_Reagan]   \n",
       "4             100006       SUPPORTS                    [Samoa_Joe]   \n",
       "...              ...            ...                            ...   \n",
       "97952          99990       SUPPORTS                 [The_Simpsons]   \n",
       "97953          99992        REFUTES              [William_Shatner]   \n",
       "97954          99995       SUPPORTS                [Pablo_Fenjves]   \n",
       "97955          99997       SUPPORTS         [Dwight_D._Eisenhower]   \n",
       "97956          99999        REFUTES                        [Jesus]   \n",
       "\n",
       "                                               evidences  \\\n",
       "0      [[{'docid': 'Ireland', 'end_sentence': 7, 'end...   \n",
       "1      [[{'docid': 'Taylor_Schilling', 'end_sentence'...   \n",
       "2      [[{'docid': 'IZombie_-LRB-TV_series-RRB-', 'en...   \n",
       "3      [[{'docid': 'Ronald_Reagan', 'end_sentence': 1...   \n",
       "4      [[{'docid': 'Samoa_Joe', 'end_sentence': 1, 'e...   \n",
       "...                                                  ...   \n",
       "97952  [[{'docid': 'The_Simpsons', 'end_sentence': 12...   \n",
       "97953  [[{'docid': 'William_Shatner', 'end_sentence':...   \n",
       "97954  [[{'docid': 'Pablo_Fenjves', 'end_sentence': 7...   \n",
       "97955  [[{'docid': 'Dwight_D._Eisenhower', 'end_sente...   \n",
       "97956  [[{'docid': 'Jesus', 'end_sentence': 4, 'end_t...   \n",
       "\n",
       "                                                   query  query_type  \n",
       "0      Ireland does not have relatively low-lying mou...         NaN  \n",
       "1          The drama Dark Matter stars Taylor Schilling.         NaN  \n",
       "2                             IZombie premiered in 2015.         NaN  \n",
       "3                       Ronald Reagan had a nationality.         NaN  \n",
       "4                     Samoa Joe wrestles professionally.         NaN  \n",
       "...                                                  ...         ...  \n",
       "97952     The Simpsons have aired on Fox for 29 seasons.         NaN  \n",
       "97953  William Shatner did not host a show that won a...         NaN  \n",
       "97954       Pablo Fenjves ghostwrote for a acting agent.         NaN  \n",
       "97955  The Republic of China was recognized as China'...         NaN  \n",
       "97956              Jesus did not begin his own ministry.         NaN  \n",
       "\n",
       "[97957 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_stats(train_df, test_df, val_df):\n",
    "    text_lens_0 = []\n",
    "    text_lens_1 = []\n",
    "    rationale_lens_0 = []\n",
    "    rationale_lens_1 = []\n",
    "    rationale_percent_0 = []\n",
    "    rationale_percent_1 = []\n",
    "    class_distribution = [0,0]\n",
    "    for df in [train_df, test_df, val_df]:\n",
    "        for i in range(len(df)):\n",
    "            df_row = df.loc[i]\n",
    "            clas = df_row['classification']\n",
    "            text = df_row['text']\n",
    "            rationale = df_row['rationale']\n",
    "            query = df_row['query']\n",
    "            \n",
    "            query_len = len(query.split())\n",
    "            text_len = len(text.split()) - query_len - 1\n",
    "            rationale_len = rationale.count(1) - query_len - 1\n",
    "            rationale_percent = rationale_len/text_len\n",
    "            if clas == \"REFUTES\":\n",
    "                text_lens_0.append(text_len)\n",
    "                rationale_lens_0.append(rationale_len)\n",
    "                rationale_percent_0.append(rationale_percent)\n",
    "                class_distribution[0] += 1\n",
    "            else:\n",
    "                text_lens_1.append(text_len)\n",
    "                rationale_lens_1.append(rationale_len)\n",
    "                rationale_percent_1.append(rationale_percent)\n",
    "                class_distribution[1] += 1\n",
    "                \n",
    "    all_stats = {\"text_lens_0\": text_lens_0,\n",
    "                 \"text_lens_1\": text_lens_1,\n",
    "                 \"text_lens_all\":text_lens_0 + text_lens_1,\n",
    "                 \"rationale_lens_0\":rationale_lens_0,\n",
    "                 \"rationale_lens_1\":rationale_lens_1,\n",
    "                 \"rationale_lens_all\":rationale_lens_0 + rationale_lens_1,\n",
    "                 \"rationale_percent_0\": rationale_percent_0,\n",
    "                 \"rationale_percent_1\": rationale_percent_1,\n",
    "                 \"rationale_percent_all\": rationale_percent_0 + rationale_percent_1,\n",
    "                 \"class_distr\":[class_distribution[0]/sum(class_distribution),class_distribution[1]/sum(class_distribution)]\n",
    "                }\n",
    "    return all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = generate_class_stats(train_data_df,test_data_df,val_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_lens_0: 318.38970787736275\n",
      "text_lens_1: 330.0212544956439\n",
      "text_lens_all: 326.5197567837372\n",
      "rationale_lens_0: 46.364957342256794\n",
      "rationale_lens_1: 45.63103909424947\n",
      "rationale_lens_all: 45.85197386332698\n",
      "rationale_percent_0: 0.21115993968409894\n",
      "rationale_percent_1: 0.19960476708895997\n",
      "rationale_percent_all: 0.20308327357913616\n",
      "class_distr: 0.5\n",
      "[0.3010345766403485, 0.6989654233596515]\n"
     ]
    }
   ],
   "source": [
    "for key,val in all_stats.items():\n",
    "    print(f\"{key}: {avg(val)}\")\n",
    "print(all_stats[\"class_distr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22822182250835382"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg(rationale_percent_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
