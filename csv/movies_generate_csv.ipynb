{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils import avg, evidence_to_mask\n",
    "\n",
    "def to_data_df(df, data_dir):\n",
    "    data_df = []\n",
    "    columns = ['text', 'classification', 'rationale' ,'query']\n",
    "    for i in tqdm(range(len(df))):\n",
    "        df_row = df.loc[i]\n",
    "        \n",
    "        doc_id = df_row['annotation_id']\n",
    "        query = df_row['query']\n",
    "        evidence_list = df_row['evidences']\n",
    "        if evidence_list:\n",
    "            evidence_list = evidence_list[0]\n",
    "        classification = df_row['classification']\n",
    "        \n",
    "        text = ''\n",
    "        file = f'{data_dir}/docs/{doc_id}'\n",
    "        if os.path.isfile(file):\n",
    "            f = open(file, 'r', encoding=\"utf-8\") \n",
    "            for line in f.readlines():\n",
    "                text += line.rstrip() + ' '\n",
    "        else:\n",
    "            print(\"???\")\n",
    "            print(file)\n",
    "            quit()\n",
    "        \n",
    "        tokens = text.split()\n",
    "        rationale_mask = evidence_to_mask(tokens, evidence_list)\n",
    "        \n",
    "        # joining text and query with [SEP]\n",
    "#         QA = f\"{text}\"\n",
    "#         QA = f\"{text}[SEP] {query}\"\n",
    "#         QA = f\"{query} [SEP] {text}\"\n",
    "        \n",
    "        QA = text\n",
    "        rationale_mask = rationale_mask\n",
    "    \n",
    "        data_df.append([QA, classification, rationale_mask, query])\n",
    "    data_df = pd.DataFrame(data_df, columns=columns)\n",
    "#     return data_df\n",
    "    \n",
    "    data_df_shuffled=data_df.sample(frac=1).reset_index(drop=True)\n",
    "    return data_df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"movies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'../data/{dataset}'\n",
    "train = pd.read_json(f'{data_dir}/train.jsonl', lines=True)\n",
    "test = pd.read_json(f'{data_dir}/test.jsonl', lines=True)\n",
    "val = pd.read_json(f'{data_dir}/val.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1600/1600 [00:13<00:00, 119.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 199/199 [00:02<00:00, 99.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 95.06it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_df = to_data_df(train, data_dir)\n",
    "train_data_df.to_csv(f\"{dataset}/train.csv\",index_label=\"id\")\n",
    "test_data_df = to_data_df(test, data_dir)\n",
    "test_data_df.to_csv(f\"{dataset}/test.csv\",index_label=\"id\")\n",
    "val_data_df = to_data_df(val, data_dir)\n",
    "val_data_df.to_csv(f\"{dataset}/val.csv\",index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text              edward zwick 's \" the siege \" raises more ques...\n",
       "classification                                                  POS\n",
       "rationale         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "query                         What is the sentiment of this review?\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 43,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 40,\n",
       "   'text': 'i even giggled'},\n",
       "  {'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 76,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 58,\n",
       "   'text': 'something about these films causes me to lower my inhibitions and return to the saturday afternoons of my'},\n",
       "  {'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 115,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 106,\n",
       "   'text': \"does n't quite pass the test . sure enough\"},\n",
       "  {'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 191,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 182,\n",
       "   'text': 'too - cheesy - to - be - accidental'},\n",
       "  {'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 249,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 241,\n",
       "   'text': 'noteworthy primarily for the mechanical manner in which'},\n",
       "  {'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 352,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 320,\n",
       "   'text': \"it 's hard to work up much enthusiasm for this sort of joyless film - making , especially when a monster moview should make you laugh every time it makes you scream\"},\n",
       "  {'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 442,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 419,\n",
       "   'text': \"deep rising is missing that one unmistakable cue that we 're expected to have a ridiculous good time , not hide our eyes\"},\n",
       "  {'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 557,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 530,\n",
       "   'text': \"deep rising has anaconda beat all to heck when it comes to technical proficiency and pacing . it 's also gloomy , uninspired and not nearly enough\"},\n",
       "  {'docid': 'negR_900.txt',\n",
       "   'end_sentence': -1,\n",
       "   'end_token': 614,\n",
       "   'start_sentence': -1,\n",
       "   'start_token': 559,\n",
       "   'text': \"i do n't ask much of my monster movies , but i do ask that they act like monster movies . you do n't have to show me a fantastically impressive , massive beast with tentacles a - flailing . just show me the massive beast burping , and i 'll figure you get the\"}]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[0]['evidences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_data_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "even\n",
      "giggled\n",
      "something\n",
      "about\n",
      "these\n",
      "films\n",
      "causes\n",
      "me\n",
      "to\n",
      "lower\n",
      "my\n",
      "inhibitions\n",
      "and\n",
      "return\n",
      "to\n",
      "the\n",
      "saturday\n",
      "afternoons\n",
      "of\n",
      "my\n",
      "does\n",
      "n't\n",
      "quite\n",
      "pass\n",
      "the\n",
      "test\n",
      ".\n",
      "sure\n",
      "enough\n",
      "too\n",
      "-\n",
      "cheesy\n",
      "-\n",
      "to\n",
      "-\n",
      "be\n",
      "-\n",
      "accidental\n",
      "noteworthy\n",
      "primarily\n",
      "for\n",
      "the\n",
      "mechanical\n",
      "manner\n",
      "in\n",
      "which\n",
      "it\n",
      "'s\n",
      "hard\n",
      "to\n",
      "work\n",
      "up\n",
      "much\n",
      "enthusiasm\n",
      "for\n",
      "this\n",
      "sort\n",
      "of\n",
      "joyless\n",
      "film\n",
      "-\n",
      "making\n",
      ",\n",
      "especially\n",
      "when\n",
      "a\n",
      "monster\n",
      "moview\n",
      "should\n",
      "make\n",
      "you\n",
      "laugh\n",
      "every\n",
      "time\n",
      "it\n",
      "makes\n",
      "you\n",
      "scream\n",
      "deep\n",
      "rising\n",
      "is\n",
      "missing\n",
      "that\n",
      "one\n",
      "unmistakable\n",
      "cue\n",
      "that\n",
      "we\n",
      "'re\n",
      "expected\n",
      "to\n",
      "have\n",
      "a\n",
      "ridiculous\n",
      "good\n",
      "time\n",
      ",\n",
      "not\n",
      "hide\n",
      "our\n",
      "eyes\n",
      "deep\n",
      "rising\n",
      "has\n",
      "anaconda\n",
      "beat\n",
      "all\n",
      "to\n",
      "heck\n",
      "when\n",
      "it\n",
      "comes\n",
      "to\n",
      "technical\n",
      "proficiency\n",
      "and\n",
      "pacing\n",
      ".\n",
      "it\n",
      "'s\n",
      "also\n",
      "gloomy\n",
      ",\n",
      "uninspired\n",
      "and\n",
      "not\n",
      "nearly\n",
      "enough\n",
      "i\n",
      "do\n",
      "n't\n",
      "ask\n",
      "much\n",
      "of\n",
      "my\n",
      "monster\n",
      "movies\n",
      ",\n",
      "but\n",
      "i\n",
      "do\n",
      "ask\n",
      "that\n",
      "they\n",
      "act\n",
      "like\n",
      "monster\n",
      "movies\n",
      ".\n",
      "you\n",
      "do\n",
      "n't\n",
      "have\n",
      "to\n",
      "show\n",
      "me\n",
      "a\n",
      "fantastically\n",
      "impressive\n",
      ",\n",
      "massive\n",
      "beast\n",
      "with\n",
      "tentacles\n",
      "a\n",
      "-\n",
      "flailing\n",
      ".\n",
      "just\n",
      "show\n",
      "me\n",
      "the\n",
      "massive\n",
      "beast\n",
      "burping\n",
      ",\n",
      "and\n",
      "i\n",
      "'ll\n",
      "figure\n",
      "you\n",
      "get\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "te = t['text'].split()\n",
    "for i, b in enumerate(t['rationale']):\n",
    "    if b:\n",
    "        print(te[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_stats(train_df, test_df, val_df):\n",
    "    text_lens_0 = []\n",
    "    text_lens_1 = []\n",
    "    rationale_lens_0 = []\n",
    "    rationale_lens_1 = []\n",
    "    rationale_percent_0 = []\n",
    "    rationale_percent_1 = []\n",
    "    class_distribution = [0,0]\n",
    "    for df in [train_df, test_df, val_df]:\n",
    "        for i in range(len(df)):\n",
    "            df_row = df.loc[i]\n",
    "            clas = df_row['classification']\n",
    "            text = df_row['text']\n",
    "            rationale = df_row['rationale']\n",
    "            text_len = len(text.split())\n",
    "            rationale_len = rationale.count(1)\n",
    "            rationale_percent = rationale_len/text_len\n",
    "            if clas == \"NEG\":\n",
    "                text_lens_0.append(text_len)\n",
    "                rationale_lens_0.append(rationale_len)\n",
    "                rationale_percent_0.append(rationale_percent)\n",
    "                class_distribution[0] += 1\n",
    "            else:\n",
    "                text_lens_1.append(text_len)\n",
    "                rationale_lens_1.append(rationale_len)\n",
    "                rationale_percent_1.append(rationale_percent)\n",
    "                class_distribution[1] += 1\n",
    "    return text_lens_0,text_lens_1,rationale_lens_0,rationale_lens_1,rationale_percent_0,rationale_percent_1,class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lens_0,text_lens_1,rationale_lens_0,rationale_lens_1,rationale_percent_0,rationale_percent_1,class_distribution = generate_class_stats(train_data_df,test_data_df,val_data_df)\n",
    "text_lens_all = text_lens_0 + text_lens_1\n",
    "rationale_lens_all = rationale_lens_0 + rationale_lens_1\n",
    "rationale_percent_all = rationale_percent_0 + rationale_percent_1\n",
    "class_distr = [class_distribution[0]/sum(class_distribution),class_distribution[1]/sum(class_distribution)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.925462731365684\n",
      "774.2696348174087\n",
      "0.04227329387265028\n",
      "731.786\n",
      "816.7957957957958\n",
      "36.84\n",
      "25.005005005005007\n",
      "0.052422732743648545\n",
      "0.03211369540318255\n"
     ]
    }
   ],
   "source": [
    "for l in [rationale_lens_all,text_lens_all,rationale_percent_all,text_lens_0,text_lens_1,rationale_lens_0,rationale_lens_1,rationale_percent_0,rationale_percent_1]:\n",
    "    print(avg(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5002501250625313, 0.49974987493746875]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
