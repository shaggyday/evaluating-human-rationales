{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from utils import avg, evidence_to_mask\n",
    "\n",
    "def to_data_df(df, data_dir):\n",
    "    data_df = []\n",
    "    columns = ['text', 'classification', 'rationale', 'query']\n",
    "    for i in tqdm(range(len(df))):\n",
    "        df_row = df.loc[i]\n",
    "        text_id = df_row['annotation_id']\n",
    "        idx = text_id.find('txt')\n",
    "        text_id = text_id[:idx+3]\n",
    "        query = df_row['query']\n",
    "        evidence_list = df_row['evidences']\n",
    "        if evidence_list:\n",
    "            evidence_list = [x for xx in evidence_list for x in xx]\n",
    "        classification = df_row['classification']\n",
    "        \n",
    "        file = f'{data_dir}/docs/{text_id}'\n",
    "        if os.path.isfile(file):\n",
    "            f = open(file, 'r', encoding=\"utf-8\") \n",
    "            text = ''\n",
    "            for line in f.readlines():\n",
    "                text += line.rstrip() + ' '\n",
    "        else:\n",
    "            print(\"???\")      \n",
    "        \n",
    "        tokens = text.split()\n",
    "        rationale_mask = evidence_to_mask(tokens, evidence_list)\n",
    "        \n",
    "######## THIS WORKS!!!!!!!!!##############\n",
    "#         query = query.replace(\"||\",\"[SEP]\")\n",
    "#         QA = f\"{text} {query}\"\n",
    "#         rationale_mask = rationale_mask + [1]*(len(query.split())+1)\n",
    "        \n",
    "        query = query.split(\"||\")\n",
    "        QA = f\"{text} {query[0]}\"\n",
    "#         rationale_mask = rationale_mask + [1]*(len(query.split())+1)\n",
    "        \n",
    "        data_df.append([QA, classification, rationale_mask, query[1]])\n",
    "    data_df = pd.DataFrame(data_df, columns=columns)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"multirc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'../data/{dataset}'\n",
    "train = pd.read_json(f'{data_dir}/train.jsonl', lines=True)\n",
    "test = pd.read_json(f'{data_dir}/test.jsonl', lines=True)\n",
    "val = pd.read_json(f'{data_dir}/val.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████▉                                                                | 3512/24029 [00:02<00:17, 1203.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f10c3e0e2901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_data_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_data_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{dataset}/train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_data_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_data_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{dataset}/test.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_data_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_data_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-d4ad24b21c70>\u001b[0m in \u001b[0;36mto_data_df\u001b[1;34m(df, data_dir)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{data_dir}/docs/{text_id}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data_df = to_data_df(train, data_dir)\n",
    "train_data_df.to_csv(f\"{dataset}/train.csv\",index_label=\"id\")\n",
    "test_data_df = to_data_df(test, data_dir)\n",
    "test_data_df.to_csv(f\"{dataset}/test.csv\",index_label=\"id\")\n",
    "val_data_df = to_data_df(val, data_dir)\n",
    "val_data_df.to_csv(f\"{dataset}/val.csv\",index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rationale_check(text,rationale):\n",
    "    tokens = text.split()\n",
    "    out = \"\"\n",
    "    for i, b in enumerate(rationale):\n",
    "        if b:\n",
    "           out += tokens[i] + \" \"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = test_data_df\n",
    "import json\n",
    "\n",
    "def reduce_by_alpha(text, rationale, fidelity_type=\"sufficiency\"):\n",
    "    reduced_text = \"\"\n",
    "    # whitespace tokenization\n",
    "    tokens = text.split()\n",
    "\n",
    "    for idx in range(len(tokens)):\n",
    "        try:\n",
    "            if fidelity_type == \"sufficiency\" and rationale[idx] >= 0.5:\n",
    "                reduced_text = reduced_text + tokens[idx] + \" \"\n",
    "            elif fidelity_type == \"comprehensiveness\" and rationale[idx] < 0.5:\n",
    "                reduced_text = reduced_text + tokens[idx] + \" \"\n",
    "        except Exception as e:\n",
    "            if fidelity_type == \"comprehensiveness\":\n",
    "                reduced_text = reduced_text + tokens[idx] + \" \"\n",
    "\n",
    "    # removed the last space from the text\n",
    "    if len(reduced_text) > 0:\n",
    "        reduced_text = reduced_text[:-1]\n",
    "\n",
    "    return reduced_text\n",
    "\n",
    "data_df = data_df[data_df['rationale'].notna()]\n",
    "data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "data_df[\"sufficiency_text\"] = data_df[\n",
    "    [\"text\", \"rationale\"]].apply(lambda s: reduce_by_alpha(*s, fidelity_type=\"sufficiency\"), axis=1)\n",
    "data_df[\"comprehensiveness_text\"] = data_df[\n",
    "    [\"text\", \"rationale\"]].apply(lambda s: reduce_by_alpha(*s, fidelity_type=\"comprehensiveness\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was these same impulses , leading so invariably to success , that made his enemies call him the Wisest Man . He leaned forward and touched the chauffeur \\'s shoulder . \" Stop at the Court of General Sessions , \" he commanded . A word , a personal word from him to the district attorney , or the judge , would be enough . '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rationale_check(train_data_df.iloc[2]['text'],train_data_df.iloc[2]['rationale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'docid': 'Fiction-stories-masc-A_Wasted_Day-2.txt',\n",
       "   'end_sentence': 16,\n",
       "   'end_token': 349,\n",
       "   'start_sentence': 15,\n",
       "   'start_token': 327,\n",
       "   'text': 'It was these same impulses , leading so invariably to success , that made his enemies call him the Wisest Man .'},\n",
       "  {'docid': 'Fiction-stories-masc-A_Wasted_Day-2.txt',\n",
       "   'end_sentence': 17,\n",
       "   'end_token': 359,\n",
       "   'start_sentence': 16,\n",
       "   'start_token': 349,\n",
       "   'text': \"He leaned forward and touched the chauffeur 's shoulder .\"},\n",
       "  {'docid': 'Fiction-stories-masc-A_Wasted_Day-2.txt',\n",
       "   'end_sentence': 18,\n",
       "   'end_token': 372,\n",
       "   'start_sentence': 17,\n",
       "   'start_token': 359,\n",
       "   'text': '\" Stop at the Court of General Sessions , \" he commanded .'},\n",
       "  {'docid': 'Fiction-stories-masc-A_Wasted_Day-2.txt',\n",
       "   'end_sentence': 20,\n",
       "   'end_token': 405,\n",
       "   'start_sentence': 19,\n",
       "   'start_token': 384,\n",
       "   'text': 'A word , a personal word from him to the district attorney , or the judge , would be enough .'}]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['evidences'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_stats(train_df, test_df, val_df):\n",
    "    text_lens_0 = []\n",
    "    text_lens_1 = []\n",
    "    rationale_lens_0 = []\n",
    "    rationale_lens_1 = []\n",
    "    rationale_percent_0 = []\n",
    "    rationale_percent_1 = []\n",
    "    class_distribution = [0,0]\n",
    "    for df in [train_df, test_df, val_df]:\n",
    "        for i in range(len(df)):\n",
    "            df_row = df.loc[i]\n",
    "            clas = df_row['classification']\n",
    "            text = df_row['text']\n",
    "            rationale = df_row['rationale']\n",
    "            query = df_row['query']\n",
    "            \n",
    "            query_len = len(query.split())\n",
    "            text_len = len(text.split())\n",
    "            rationale_len = rationale.count(1)\n",
    "            rationale_percent = rationale_len/text_len\n",
    "            if clas == \"False\":\n",
    "                text_lens_0.append(text_len)\n",
    "                rationale_lens_0.append(rationale_len)\n",
    "                rationale_percent_0.append(rationale_percent)\n",
    "                class_distribution[0] += 1\n",
    "            else:\n",
    "                text_lens_1.append(text_len)\n",
    "                rationale_lens_1.append(rationale_len)\n",
    "                rationale_percent_1.append(rationale_percent)\n",
    "                class_distribution[1] += 1\n",
    "                \n",
    "    all_stats = {\"text_lens_0\": text_lens_0,\n",
    "                 \"text_lens_1\": text_lens_1,\n",
    "                 \"text_lens_all\":text_lens_0 + text_lens_1,\n",
    "                 \"rationale_lens_0\":rationale_lens_0,\n",
    "                 \"rationale_lens_1\":rationale_lens_1,\n",
    "                 \"rationale_lens_all\":rationale_lens_0 + rationale_lens_1,\n",
    "                 \"rationale_percent_0\": rationale_percent_0,\n",
    "                 \"rationale_percent_1\": rationale_percent_1,\n",
    "                 \"rationale_percent_all\": rationale_percent_0 + rationale_percent_1,\n",
    "                 \"class_distr\":[class_distribution[0]/sum(class_distribution),class_distribution[1]/sum(class_distribution)]\n",
    "                }\n",
    "    return all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_lens_0: 318.45183702962595\n",
      "text_lens_1: 310.77560283687944\n",
      "text_lens_all: 315.0790875946527\n",
      "rationale_lens_0: 50.528708798843866\n",
      "rationale_lens_1: 50.67531914893617\n",
      "rationale_lens_all: 50.593125798510485\n",
      "rationale_percent_0: 0.16558116676322926\n",
      "rationale_percent_1: 0.16995819701331757\n",
      "rationale_percent_all: 0.16750432673101273\n",
      "class_distr: 0.5\n",
      "[0.5606244741516313, 0.4393755258483687]\n"
     ]
    }
   ],
   "source": [
    "all_stats = generate_class_stats(train_data_df,test_data_df,val_data_df)\n",
    "for key,value in all_stats.items():\n",
    "    print(f\"{key}: {avg(value)}\")\n",
    "print(all_stats[\"class_distr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Allan sat down at his desk and pulled the chair in close . Opening a side drawer , he took out a piece of paper and his inkpot . After filling his pen , Allan looked at his paper in the orange glow from the lantern set back in the desk \\'s right - hand corner . His pen cast a forbidding line of shadow slanting across the page , echoing the inky darkness crouching in the edges of the lantern \\'s struggling glow . The only other illumination came from a lurid moonlight filtered through thin branches and clouds , casting its bone - pale glow onto the pine floorboards . Allan unfolded another page , this one crowded with ranks of letters in tight formation from left to right . The lines of letters stepped into their divisions , in the shape of a story \\'s outline : the loose , dry skeleton of a tale lay exposed beneath their feet , awaiting tendons , muscle and blushing skin . Allan reviewed the troops , all prepared to disembark , their task to form the tale of a young man returning home from Life Abroad to find his childhood friend a bride to - be , thus upsetting the apple cart of his life \\'s plan , clarified – of course – by his very time away from her he loved best . Although the concept was a simple one , Allan thought it had potential . Besides , the public liked a good , simple romance . Perhaps this will be more saleable , he thought and began to write . They gazed at each other , lost in the rapture of love based so deeply within their hearts that they had never seen it before . \" What about Roger ? \" she asked , knowing that the answer no longer mattered .  Name few objects said to be in or on Allan \\'s desk '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   annotation_id   3214 non-null   object \n",
      " 1   classification  3214 non-null   object \n",
      " 2   docids          0 non-null      float64\n",
      " 3   evidences       3214 non-null   object \n",
      " 4   query           3214 non-null   object \n",
      " 5   query_type      0 non-null      float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 150.8+ KB\n"
     ]
    }
   ],
   "source": [
    "val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13456"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(c).count(\"False\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
