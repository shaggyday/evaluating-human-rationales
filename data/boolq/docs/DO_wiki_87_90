TYPE I AND TYPE II ERRORS
In statistical hypothesis testing , a type I error is the rejection of a true null hypothesis ( also known as a " false positive " finding ) , while a type II error is failing to reject a false null hypothesis ( also known as a " false negative " finding ) .
More simply stated , a type I error is to falsely infer the existence of something that is not there , while a type II error is to falsely infer the absence of something that is present .
DEFINITION Section::::Definition .
In statistics , a null hypothesis is a statement that one seeks to nullify with evidence to the contrary .
Most commonly it is a statement that the phenomenon being studied produces no effect or makes no difference .
An example of a null hypothesis is the statement
" This diet has no effect on people 's weight . "
Usually , an experimenter frames a null hypothesis with the intent of rejecting it : that is , intending to run an experiment which produces data that shows that the phenomenon under study does make a difference .
In some cases there is a specific alternative hypothesis that is opposed to the null hypothesis , in other cases the alternative hypothesis is not explicitly stated , or is simply " the null hypothesis is false " – in either event , this is a binary judgment , but the interpretation differs and is a matter of significant dispute in statistics .
A type I error ( or error of the first kind ) is the incorrect rejection of a true null hypothesis .
Usually a type I error leads one to conclude that a supposed effect or relationship exists when in fact it does n't .
Examples of type
I errors include a test that shows a patient to have a disease when in fact the patient does not have the disease , a fire alarm going on indicating a fire when in fact there is no fire , or an experiment indicating that a medical treatment should cure a disease when in fact it does not .
A type II error ( or error of the second kind ) is the failure to reject a false null hypothesis .
Examples of type II errors would be a blood test failing to detect the disease it was designed to detect , in a patient who really has the disease ; a fire breaking out and the fire alarm does not ring ; or a clinical trial of a medical treatment failing to show that the treatment works when really it does .
In terms of false positives and false negatives , a positive result corresponds to rejecting the null hypothesis , while a negative result corresponds to failing to reject the null hypothesis ; " false " means the conclusion drawn is incorrect .
Thus a type I error is a false positive , and a type II error is a false negative .
When comparing two means , concluding the means were different when in reality they were not different would be a Type I error ; concluding the means were not different when in reality they were different would be a Type II error .
Various extensions have been suggested as " Type III errors " , though none have wide use .
In practice , the difference between a false positive and false negative is usually not obvious , since all statistical hypothesis tests have a probability of making type I and type II errors .
For example , all blood tests for a disease will falsely detect the disease in some proportion of people who do n't have it , and will fail to detect the disease in some proportion of people who do have it .
A test 's probability of making a type I error is denoted by α .
A test 's probability of making a type II error is denoted by β .
These error rates are traded off against each other : for any given sample set , the effort to reduce one type of error generally results in increasing the other type of error .
For a given test , the only way to reduce both error rates is to increase the sample size , and this may not be feasible .
A test statistic is robust if the Type I error rate is controlled .
How likely the results are to be found if the null hypothesis is true is called statistical significance .
These terms are also used in a more general way by social scientists and others to refer to flaws in reasoning .
This article is specifically devoted to the statistical meanings of those terms and the technical issues of the statistical errors that those terms describe .
STATISTICAL TEST THEORY Section::::Statistical test theory .
In statistical test theory , the notion of statistical error is an integral part of hypothesis testing .
The test requires an unambiguous statement of a null hypothesis , which usually corresponds to a default " state of nature " , for example " this person is healthy " , " this accused is not guilty " or " this product is not broken " .
An alternative hypothesis is the negation of null hypothesis , for example , " this person is not healthy " , " this accused is guilty " or " this product is broken " .
The result of the test may be negative , relative to the null hypothesis ( not healthy , guilty , broken ) or positive ( healthy , not guilty , not broken ) .
If the result of the test corresponds with reality , then a correct decision has been made .
However , if the result of the test does not correspond with reality , then an error has occurred .
Due to the statistical nature of a test , the result is never , except in very rare cases , free of error .
Two types of error are distinguished : type
I error and type II error .
TYPE
I ERROR Section::::Type
I error .
A type I error occurs when the null hypothesis ( H ) is true , but is rejected .
It is asserting something that is absent , a false hit .
A type I error may be likened to a so - called false positive ( a result that indicates that a given condition is present when it actually is not present ) .
In terms of folk tales , an investigator may see the wolf when there is none ( " raising a false alarm " ) .
Where the null hypothesis , H , is : no wolf .
The type I error rate or significance level is the probability of rejecting the null hypothesis given that it is true .
It is denoted by the Greek letter α ( alpha ) and is also called the alpha level .
Often , the significance level is set to 0.05 ( 5 % ) , implying that it is acceptable to have a 5 % probability of incorrectly rejecting the null hypothesis .
TYPE II ERROR Section::::Type II error .
A type II error occurs when the null hypothesis is false , but erroneously fails to be rejected .
It is failing to assert what is present , a miss .
A type II error may be compared with a so - called false negative ( where an actual ' hit ' was disregarded by the test and seen as a ' miss ' ) in a test checking for a single condition with a definitive result of true or false .
A Type II error is committed when we fail to believe a true alternative hypothesis .
In terms of folk tales , an investigator may fail to see the wolf when it is present ( " failing to raise an alarm " ) .
Again , H : no wolf .
The rate of the type II error is denoted by the Greek letter β ( beta ) and related to the power of a test ( which equals 1−β ) .
TABLE OF ERROR TYPES Section::::Table of error types .
Tabularised relations between truth / falseness of the null hypothesis and outcomes of the test : !
rowspan="2 " colspan="2 " Table of error types !
colspan="2 " Null hypothesis ( H ) is !
True !
False !
rowspan="2 " DecisionAbout NullHypothesis ( H ) !
Fail toreject ( Probability = 1 - α ) !
Reject ( Probability = 1 - β ) EXAMPLES EXAMPLE 1 Section::::Examples . Section::::Example 1 .
Hypothesis : " Adding water to toothpaste protects against cavities . "
Null hypothesis ( H ) :
" Adding water does not make toothpaste more effective in fighting cavities . "
This null hypothesis is tested against experimental data with a view to nullifying it with evidence to the contrary .
A type I error occurs when detecting an effect ( adding water to toothpaste protects against cavities )
that is not present .
The null hypothesis is true ( i.e. , it is true that adding water to toothpaste does not make it more effective in protecting against cavities ) , but this null hypothesis is rejected based on bad experimental data or an extreme outcome of chance alone .
EXAMPLE 2 Section::::Example 2 .
Hypothesis : " Adding fluoride to toothpaste protects against cavities . "
Null hypothesis ( H ) :
" Adding fluoride to toothpaste has no effect on cavities . "
This null hypothesis is tested against experimental data with a view to nullifying it with evidence to the contrary .
A type II error occurs when failing to detect an effect ( adding fluoride to toothpaste protects against cavities ) that is present .
The null hypothesis is false ( i.e. , adding fluoride is actually effective against cavities ) , but the experimental data is such that the null hypothesis can not be rejected .
EXAMPLE 3 Section::::Example 3 .
Hypothesis : " The evidence produced before the court proves that this man is guilty . "
Null hypothesis ( H ) :
" This man is innocent . "
A type I error occurs when convicting an innocent person ( a miscarriage of justice ) .
A type II error occurs when letting a guilty person go free ( an error of impunity ) .
A positive correct outcome occurs when convicting a guilty person .
A negative correct outcome occurs when letting an innocent person go free .
EXAMPLE 4 Section::::Example 4 .
Hypothesis :
" A patient 's symptoms improve after treatment A more rapidly than after a placebo treatment . "
Null hypothesis ( H ) :
" A patient 's symptoms after treatment A are indistinguishable from a placebo . "
A Type
I error would falsely indicate that treatment A is more effective than the placebo , whereas a Type II error would be a failure to demonstrate that treatment A is more effective than placebo even though it actually is more effective .
ETYMOLOGY Section::::Etymology .
In 1928 , Jerzy Neyman ( 1894–1981 ) and Egon Pearson ( 1895–1980 ) , both eminent statisticians , discussed the problems associated with " deciding whether or not a particular sample may be judged as likely to have been randomly drawn from a certain population " : and , as Florence Nightingale David remarked , " it is necessary to remember the adjective ' random ' [ in the term ' random sample ' ] should apply to the method of drawing the sample and not to the sample itself " .
They identified " two sources of error " , namely :
In 1930 , they elaborated on these two sources of error , remarking that : In 1933 , they observed that these " problems are rarely presented in such a form that we can discriminate with certainty between the true and false hypothesis " ( p. 187 ) .
They also noted that , in deciding whether to fail to reject , or reject a particular hypothesis amongst a " set of alternative hypotheses " ( p. 201 ) , H , H , . . . , it was easy to make an error : In all of the papers co - written by Neyman and Pearson the expression H always signifies " the hypothesis to be tested " .
In the same paper they call these two sources of error , errors of type I and errors of type II respectively .
RELATED TERMS
NULL
HYPOTHESIS
Section::::Related terms .
Section::::Null hypothesis .
It is standard practice for statisticians to conduct tests in order to determine whether or not a " speculative hypothesis " concerning the observed phenomena of the world ( or its inhabitants ) can be supported .
The results of such testing determine whether a particular set of results agrees reasonably ( or does not agree ) with the speculated hypothesis .
On the basis that it is always assumed , by statistical convention , that the speculated hypothesis is wrong , and the so - called " null hypothesis " that the observed phenomena simply occur by chance ( and that , as a consequence , the speculated agent has no effect ) – the test will determine whether this hypothesis is right or wrong .
This is why the hypothesis under test is often called the null hypothesis ( most likely , coined by Fisher ( 1935 , p. 19 ) ) , because it is this hypothesis that is to be either nullified or not nullified by the test .
When the null hypothesis is nullified , it is possible to conclude that data support the " alternative hypothesis " ( which is the original speculated one ) .
The consistent application by statisticians of Neyman and Pearson 's convention of representing " the hypothesis to be tested " ( or " the hypothesis to be nullified " ) with the expression H has led to circumstances where many understand the term " the null hypothesis " as meaning " the nil hypothesis " – a statement that the results in question have arisen through chance .
This is not necessarily the case – the key restriction , as per Fisher ( 1966 ) , is that " the null hypothesis must be exact , that is free from vagueness and ambiguity , because it must supply the basis of the ' problem of distribution , ' of which the test of significance is the solution . "
As a consequence of this , in experimental science the null hypothesis is generally a statement that a particular treatment has no effect ; in observational science , it is that there is no difference between the value of a particular measured variable , and that of an experimental prediction .
STATISTICAL SIGNIFICANCE
Section::::Statistical significance .
If the probability of obtaining a result as extreme as the one obtained , supposing that the null hypothesis were true , is lower than a pre - specified cut - off probability ( for example , 5 % ) , then the result is said to be statistically significant and the null hypothesis is rejected .
British statistician
Sir Ronald Aylmer Fisher ( 1890–1962 ) stressed that the " null hypothesis " : APPLICATION DOMAINS Section::::Application domains .
Statistical tests always involve a trade - off between : * the acceptable level of false positives ( in which a non - match is declared to be a match ) and * * the acceptable level of false negatives ( in which an actual match is not detected ) .
*
A threshold value can be varied to make the test more restrictive or more sensitive , with the more restrictive tests increasing the risk of rejecting true positives , and the more sensitive tests increasing the risk of accepting false positives .
INVENTORY CONTROL Section::::Inventory control .
An automated inventory control system that rejects high - quality goods of a consignment commits a type I error , while a system that accepts low - quality goods commits a type II error .
COMPUTERS Section::::Computers .
The notions of false positives and false negatives have a wide currency in the realm of computers and computer applications , as follows .
COMPUTER SECURITY Section::::Computer security .
Security vulnerabilities are an important consideration in the task of keeping computer data safe , while maintaining access to that data for appropriate users .
Moulton ( 1983 ) , stresses the importance of : * avoiding the type I errors ( or false positives ) that classify authorized users as imposters .
* avoiding the type II errors ( or false negatives )
that classify imposters as authorized users .
SPAM FILTERING Section::::Spam filtering .
A false positive occurs when spam filtering or spam blocking techniques wrongly classify a legitimate email message as spam and , as a result , interferes with its delivery .
While most anti - spam tactics can block or filter a high percentage of unwanted emails , doing so without creating significant false - positive results is a much more demanding task .
A false negative occurs when a spam email is not detected as spam , but is classified as non - spam .
A low number of false negatives is an indicator of the efficiency of spam filtering .
MALWARE Section::::Malware .
The term " false positive " is also used when antivirus software wrongly classifies a harmless file as a virus .
The incorrect detection may be due to heuristics or to an incorrect virus signature in a database .
Similar problems can occur with antitrojan or antispyware software .
OPTICAL CHARACTER RECOGNITION Section::::Optical character recognition .
Detection algorithms of all kinds often create false positives .
Optical character recognition ( OCR ) software may detect an " a " where there are only some dots that appear to be an " a " to the algorithm being used .
SECURITY SCREENING Section::::Security screening .
False positives are routinely found every day in airport security screening , which are ultimately visual inspection systems .
The installed security alarms are intended to prevent weapons being brought onto aircraft ; yet they are often set to such high sensitivity that they alarm many times a day for minor items , such as keys , belt buckles , loose change , mobile phones , and tacks in shoes .
The ratio of false positives ( identifying an innocent traveller as a terrorist ) to true positives ( detecting a would - be terrorist ) is , therefore , very high ; and because almost every alarm is a false positive , the positive predictive value of these screening tests is very low .
The relative cost of false results determines the likelihood that test creators allow these events to occur .
As the cost of a false negative in this scenario is extremely high ( not detecting a bomb being brought onto a plane could result in hundreds of deaths )
whilst the cost of a false positive is relatively low ( a reasonably simple further inspection )
the most appropriate test is one with a low statistical specificity but high statistical sensitivity ( one that allows a high rate of false positives in return for minimal false negatives ) .
BIOMETRICS Section::::Biometrics .
Biometric matching , such as for fingerprint recognition , facial recognition or iris recognition , is susceptible to type I and type II errors .
The null hypothesis is that the input does identify someone in the searched list of people , so : * the probability of type I errors is called the " false reject rate " ( FRR ) or false non - match rate ( FNMR ) , * while the probability of type II errors is called the " false accept rate " ( FAR ) or false match rate ( FMR ) .
If the system is designed to rarely match suspects then the probability of type II errors can be called the " false alarm rate " .
On the other hand , if the system is used for validation ( and acceptance is the norm )
then the FAR is a measure of system security , while the FRR measures user inconvenience level .
MEDICINE MEDICAL SCREENING Section::::Medicine .
Section::::Medical screening .
In the practice of medicine , there is a significant difference between the applications of screening and testing .
*
Screening involves relatively cheap tests that are given to large populations , none of whom manifest any clinical indication of disease ( e.g. , Pap smears ) .
* Testing involves far more expensive , often invasive , procedures that are given only to those who manifest some clinical indication of disease , and are most often applied to confirm a suspected diagnosis .
For example , most states in the USA require newborns to be screened for phenylketonuria and hypothyroidism , among other congenital disorders .
Although they display a high rate of false positives , the screening tests are considered valuable because they greatly increase the likelihood of detecting these disorders at a far earlier stage .
The simple blood tests used to screen possible blood donors for HIV and hepatitis have a significant rate of false positives ; however , physicians use much more expensive and far more precise tests to determine whether a person is actually infected with either of these viruses .
Perhaps the most widely discussed false positives in medical screening come from the breast cancer screening procedure mammography .
The US rate of false positive mammograms is up to 15 % , the highest in world .
One consequence of the high false positive rate in the US is that , in any 10-year period , half of the American women screened receive a false positive mammogram .
False positive mammograms are costly , with over $ 100 million spent annually in the U.S. on follow - up testing and treatment .
They also cause women unneeded anxiety .
As a result of the high false positive rate in the US , as many as 90–95 % of women who get a positive mammogram do not have the condition .
The lowest rate in the world is in the Netherlands , 1 % .
The lowest rates are generally in Northern Europe where mammography films are read twice and a high threshold for additional testing is set ( the high threshold decreases the power of the test ) .
The ideal population screening test would be cheap , easy to administer , and produce zero false - negatives , if possible .
Such tests usually produce more false - positives , which can subsequently be sorted out by more sophisticated ( and expensive ) testing .
MEDICAL TESTING Section::::Medical testing .
False negatives and false positives are significant issues in medical testing .
False negatives may provide a falsely reassuring message to patients and physicians that disease is absent , when it is actually present .
This sometimes leads to inappropriate or inadequate treatment of both the patient and their disease .
A common example is relying on cardiac stress tests to detect coronary atherosclerosis , even though cardiac stress tests are known to only detect limitations of coronary artery blood flow due to advanced stenosis .
False negatives produce serious and counter - intuitive problems , especially when the condition being searched for is common .
If a test with a false negative rate of only 10 % , is used to test a population with a true occurrence rate of 70 % , many of the negatives detected by the test will be false .
False positives can also produce serious and counter - intuitive problems when the condition being searched for is rare , as in screening .
If a test has a false positive rate of one in ten thousand , but only one in a million samples ( or people ) is a true positive , most of the positives detected by that test will be false .
The probability that an observed positive result is a false positive may be calculated using Bayes ' theorem .
SEE ALSO
* Binary classification
* Detection theory
* Egon Pearson * Ethics in mathematics
* False positive paradox * Family - wise error rate * Information retrieval performance measures
* Neyman – Pearson lemma
*
Null hypothesis
* Probability of a hypothesis for Bayesian inference
* Precision and recall
* Prosecutor 's fallacy
* Prozone phenomenon
* Receiver operating characteristic * Sensitivity and specificity * Statisticians ' and engineers ' cross - reference of statistical terms
* Testing hypotheses suggested by the data * Type III error
FOOTNOTES
REFERENCES
* Betz , M.A. & Gabriel , K.R. , " Type IV Errors and Analysis of Simple Effects " , Journal of Educational Statistics , Vol.3 , No.2 , ( Summer 1978 ) , pp .
121–144 .
* David , F.N. , " A Power Function for Tests of Randomness in a Sequence of Alternatives " , Biometrika , Vol.34 , Nos.3/4 , ( December 1947 ) , pp .
335–339 .
* Fisher , R.A. , The Design of Experiments , Oliver & Boyd ( Edinburgh ) , 1935 .
* Gambrill , W. , " False Positives on Newborns ' Disease Tests Worry Parents " , Health Day , ( 5 June 2006 ) .
* Kaiser , H.F. , " Directional Statistical Decisions " , Psychological Review , Vol.67 , No.3 , ( May 1960 ) , pp .
160–167 .
* Kimball , A.W. , " Errors of the Third Kind in Statistical Consulting " , Journal of the American Statistical Association , Vol.52 , No.278 , ( June 1957 ) , pp .
133–142 .
* Lubin , A. , " The Interpretation of Significant Interaction " , Educational and Psychological Measurement , Vol.21 , No.4 , ( Winter 1961 ) , pp .
807–817 .
* Marascuilo , L.A. & Levin , J.R. , " Appropriate Post
Hoc Comparisons for Interaction and nested Hypotheses in Analysis of Variance Designs : The Elimination of Type - IV Errors " , American Educational Research Journal , Vol.7 . , No.3 , ( May 1970 ) , pp .
397–421 .
* Mitroff , I.I. & Featheringham , T.R. , " On Systemic Problem Solving and the Error of the Third Kind " , Behavioral Science , Vol.19 , No.6 , ( November 1974 ) , pp .
383–393 .
* Mosteller , F. , " A k - Sample Slippage Test for an Extreme Population " , The Annals of Mathematical Statistics , Vol.19 , No.1 , ( March 1948 ) , pp .
58–65 .
*
Moulton , R.T. , " Network Security " , Datamation , Vol.29 , No.7 , ( July 1983 ) , pp .
121–127 .
*
Raiffa , H. , Decision Analysis :
Introductory Lectures on Choices Under Uncertainty , Addison – Wesley , ( Reading ) , 1968 .
EXTERNAL LINKS
* Bias and Confounding – presentation by Nigel Paneth , Graduate School of Public Health , University of Pittsburgh