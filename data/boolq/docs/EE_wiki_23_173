NESTED RAID LEVELS
Nested RAID levels , also known as hybrid RAID , combine two or more of the standard RAID levels ( where " RAID " stands for " redundant array of independent disks " ) to gain performance , additional redundancy or both , as a result of combining properties of different standard RAID layouts .
Nested RAID levels are usually numbered using a series of numbers , where the most commonly used levels use two numbers .
The first number in the numeric designation denotes the lowest RAID level in the " stack " , while the rightmost one denotes the highest layered RAID level ; for example , RAID 50 layers the data striping of RAID 0 on top of the distributed parity of RAID 5 .
Nested RAID levels include RAID 01 , RAID 10 , RAID 100 , RAID 50 and RAID 60 , which all combine data striping with other RAID techniques ; as a result of the layering scheme , RAID 01 and RAID 10 represent significantly different nested RAID levels .
( RAID 0 + 1 ) Section::::(RAID 0 + 1 ) .
RAID 01 , also called RAID 0 + 1 , is a RAID level using a mirror of stripes , achieving both replication and sharing of data between disks .
The usable capacity of a RAID 01 array is the same as in a RAID 1 array made of the same drives , in which one half of the drives is used to mirror the other half .
formula_1 , where formula_2 is the total number of drives and formula_3 is the capacity of the smallest drive in the array .
At least four disks are required in a standard RAID 01 configuration , but larger arrays are also used .
( RAID 0 + 3 ) Section::::(RAID 0
+ 3 ) .
RAID 03 , also called RAID 0
+ 3 and sometimes RAID 53 , is similar to RAID 01 with the exception that byte - level striping with dedicated parity is used instead of mirroring .
( RAID 1 + 0 ) Section::::(RAID 1 + 0 ) .
RAID 10 , also called
RAID 1 + 0 and sometimes RAID 1&0 , is similar to RAID 01 with an exception that two used standard RAID levels are layered in the opposite order ; thus , RAID 10 is a stripe of mirrors .
RAID 10 , as recognized by the storage industry association and as generally implemented by RAID controllers , is a RAID 0 array of mirrors , which may be two- or three - way mirrors , and requires a minimum of four drives .
However , a nonstandard definition of " RAID 10 " was created for the Linux MD driver ; Linux " RAID 10 " can be implemented with as few as two disks .
Implementations supporting two disks such as Linux RAID 10 offer a choice of layouts .
Arrays of more than four disks are also possible .
According to manufacturer specifications and official independent benchmarks , in most cases RAID 10 provides better throughput and latency than all other RAID levels except RAID 0 ( which wins in throughput ) .
Thus , it is the preferable RAID level for I / O - intensive applications such as database , email , and web servers , as well as for any other use requiring high disk performance .
( RAID 5 + 0 ) Section::::(RAID 5 + 0 ) .
RAID 50 , also called RAID 5 + 0 , combines the straight block - level striping of RAID 0 with the distributed parity of RAID 5 .
As a RAID 0 array striped across RAID 5 elements , minimal RAID 50 configuration requires six drives .
On the right is an example where three collections of 120 GB RAID 5s are striped together to make 720 GB of total storage space .
One drive from each of the RAID 5 sets could fail without loss of data ; for example , a RAID 50 configuration including three RAID 5 sets can only tolerate three maximum potential drive failures .
Because the reliability of the system depends on quick replacement of the bad drive so the array can rebuild , it is common to include hot spares that can immediately start rebuilding the array upon failure .
However , this does not address the issue that the array is put under maximum strain reading every bit to rebuild the array at the time when it is most vulnerable .
RAID 50 improves upon the performance of RAID 5 particularly during writes , and provides better fault tolerance than a single RAID level does .
This level is recommended for applications that require high fault tolerance , capacity and random access performance .
As the number of drives in a RAID set increases , and the capacity of the drives increase , this impacts the fault - recovery time correspondingly as the interval for rebuilding the RAID set increases .
( RAID 6 + 0 ) Section::::(RAID 6 + 0 ) .
RAID 60 , also called RAID 6 + 0
, combines the straight block - level striping of RAID 0 with the distributed double parity of RAID 6 , resulting in a RAID 0 array striped across RAID 6 elements .
It requires at least eight disks .
( RAID 10 + 0 ) Section::::(RAID 10 + 0 ) .
RAID 100 , sometimes also called RAID 10 + 0 , is a stripe of RAID 10s .
This is logically equivalent to a wider RAID 10 array , but is generally implemented using software RAID 0 over hardware RAID 10 .
Being " striped two ways " , RAID 100 is described as a " plaid RAID " .
COMPARISON Section::::Comparison .
The following table provides an overview of some considerations for nested RAID levels .
In each case : * Space efficiency is given as an expression in terms of the number of drives , ; this expression designates a fractional value between zero and one , representing the fraction of the sum of the drives ' capacities that is available for use .
For example , if three drives are arranged in RAID 3 , this gives an array space efficiency of ; thus , if each drive in this example has a capacity of 250 GB , then the array has a total capacity of 750 GB but the capacity that is usable for data storage is only 500 GB .
*
Fault tolerance is the number of drive failures allowed , where min is the guaranteed number of failures the RAID can handle and max is the maximum possible without guaranteed failure .
* Failure rate is given as an expression in terms of the number of drives , formula_4 , and the drive failure rate ,
formula_5 ( which is assumed identical and independent for each drive ) and can be seen to be a Bernoulli trial .
For example , if each of three drives has a failure rate of 5 % over the next three years , and these drives are arranged in RAID 3 , then this gives an array failure rate over the next three years of : ! rowspan="2 " Level !
rowspan="2 " Description ! rowspan="2 " Minimum number of drives !
rowspan="2 " Space efficiency !
colspan="2 "
Fault tolerance !
rowspan="2 "
Failure rate ! rowspan="2 " Read performance !
rowspan="2 " Write performance !
Min !
Max SEE
ALSO * Non - RAID drive architectures * Non - standard RAID levels NOTES REFERENCES FURTHER READING